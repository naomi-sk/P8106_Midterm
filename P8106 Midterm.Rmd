---
title: "P8106 Midterm"
author:
- "Naomi Simon-Kumar, Ila Kanneboyina, Shayne Estill"
date: "03/24/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

## Libraries

```{r}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(mgcv)
library(tidymodels)
library(earth)
library(boot) 
library(table1)
library(knitr)
library(pls)

```

## Partition into training and testing set

```{r}

# Load training data
load("dat1.RData") 
training_data <- dat1

# Load test data (dat2)
load("dat2.RData") 
testing_data <- dat2

# Convert categorical variables to factor with meaningful labels
training_data <- training_data %>%
  mutate(gender = factor(gender, 
                         levels = c(0, 1), 
                         labels = c("Female", "Male")), 
    race = factor(as.character(race), 
                  levels = c("1", "2", "3", "4"), 
                  labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(as.character(smoking), levels = c("0", "1", "2"), 
                     labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, 
                      levels = c(0, 1), 
                      labels = c("No", "Yes")),
    hypertension = factor(hypertension, 
                          levels = c(0, 1), 
                          labels = c("No", "Yes")))

# Check structure of dataset
str(training_data)
                          
# Set seed for reproducibility
set.seed(299)

# Set 10-fold cross-validation
ctrl1 <- trainControl(method = "cv", number = 10)

# Remove non-predictor variables
training_data <- training_data %>% select(-id) # remove ID variable 
testing_data <- testing_data %>% select(-id) # remove ID variable 


## Correlation Plot

# Matrix of predictors 
x <- model.matrix(log_antibody ~ ., training_data)[, -1]

# Vector of response
y <- training_data$log_antibody

# Produce corrplot
corrplot::corrplot(cor(x), method = 'circle', type = 'full')

```

## Exploratory Analysis

```{r}

## Set the plotting theme
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

## Feature Plots for numeric predictors (L2)
featurePlot(
  x = training_data[, c("age", "height", "weight", "bmi", "SBP", "LDL", "time")],
  y = training_data$log_antibody,
  plot = "scatter",
  span = 0.5,
  labels = c("Predictors", "Log Antibody"),
  type = c("p", "smooth"),
  layout = c(3, 2)
)

## Histograms for numeric predictors (need to ref code)

# Age histogram
h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  theme(plot.title = element_text(hjust = 0.5))

# BMI histogram
h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  theme(plot.title = element_text(hjust = 0.5))

# SBP histogram
h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic") +
  theme(plot.title = element_text(hjust = 0.5))

# LDL histogram
h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL") +
  theme(plot.title = element_text(hjust = 0.5))

# Time since vaccination histogram
h5 <- ggplot(training_data, aes(x = time)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Time Since Vaccination") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine using patchwork
(h1 + h2) / (h3 + h4) / (h5 + plot_spacer())


## Boxplots for categorical predictors

p1 <- ggplot(training_data, aes(x = factor(gender), y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Gender (0 = Female, 1 = Male)", y = NULL) +
  theme_bw()

p2 <- ggplot(training_data, aes(x = smoking, y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Smoking Status", y = NULL) +
  theme_bw()

p3 <- ggplot(training_data, aes(x = race, y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Race", y = NULL) +
  theme_bw()

p4 <- ggplot(training_data, aes(x = factor(diabetes), y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Diabetes", y = NULL) +
  theme_bw()

p5 <- ggplot(training_data, aes(x = factor(hypertension), y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Hypertension", y = NULL) +
  theme_bw()

# Using patchwork
((p1 + p2) / (p3 + p4) / (p5 + plot_spacer())) 


```


```{r table1}
# https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 

label(training_data$height) <- "Height (cm)"
label(training_data$age) <- "Age (yrs)"
label(training_data$weight) <- "Weight (kgs)"
label(training_data$bmi) <- "Body Mass Index (weight/ (height)^2)"
label(training_data$SBP) <- "Systolic blood pressure (mmHg)"
label(training_data$LDL) <- "LDL cholestrol (mg/dL)"
label(training_data$time) <- "Time since vaccination (in days)"
label(training_data$gender) <- "Gender"
label(training_data$race) <- "Race"
label(training_data$smoking) <- "Smoking status"
label(training_data$diabetes) <- "Diabetes"
label(training_data$hypertension) <- "Hypertension"



table1= table1(~ gender + race + age + height + weight + bmi + SBP + LDL+ time + 
         smoking + diabetes + hypertension, data=training_data)
knitr::kable(table1)

```



Based on the exploratory analysis of continuous predictors, time since vaccination appears to be right-skewed.

# Model Selection

## Linear Regression - Ila
```{r}
# Set seed for reproducibility
set.seed(299)

# linear regression
ctrl1 <- trainControl(method = "cv", number = 10)

# Fit linear model
lm.fit <- train(log_antibody ~ ., 
                data = training_data, 
                method = "lm", 
                trControl = ctrl1)

# Make predictions
lm.pred <- predict(lm.fit, newdata = testing_data)

# Calculate lm RMSE 
lm_rmse <- RMSE(lm.pred, testing_data$log_antibody)
lm_rmse

# model summary
summary_model <- summary(lm.fit$finalModel)
print(summary_model)

```
The first model we test is a simple linear model. Looking at the pdp plots none of the variables seem to show a linear trend indicating that a simple linear model is not going to be the best model and there may be more complexities.





#Elastic Net - Shayne 
```{r}
set.seed(299)
enet_fit <- train(log_antibody ~ ., data = training_data, method = "glmnet",
            tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
            lambda = exp(seq(4, -2, length = 100))),
            trControl = ctrl1)
```

```{r}
enet_fit$resample
```

The r-squared value is relatively low, thus providing evidence that this may not be a great model. 

#PCR
```{r}
set.seed(299)
pcr_mod <- pcr(log_antibody ~ .,
data = training_data,
scale = TRUE, # scale = FALSE by default
validation = "CV")
summary(pcr_mod)


# plot cross-validated mean squared error (MSEP)
validationplot(pcr_mod, val.type = "MSEP", legendpos = "topright")
```

```{r}
# determine the optimal number of components
cv_mse <- RMSEP(pcr_mod)
ncomp_cv <- which.min(cv_mse$val[1,,]) - 1
ncomp_cv
```

```{r}
# calculate test MSE
y2 = testing_data$log_antibody

predy2_pcr <- predict(pcr_mod, newdata = testing_data,
ncomp = ncomp_cv)
mean((y2 - predy2_pcr)^2)
```
The PCR MSE is 0.3592698




##PCR using caret
```{r}
x <- model.matrix(log_antibody ~ ., training_data)
y <- training_data$log_antibody

x2 <- model.matrix(log_antibody ~ .,testing_data)
y2 <- testing_data$log_antibody
```

```{r}
set.seed(299)
pcr_fit <- train(x, y,
method = "pcr",
tuneGrid = data.frame(ncomp = 1:16),
trControl = ctrl1,
preProcess = c("center", "scale"))
```

```{r}
predy2_pcr2 <- predict(pcr_fit, newdata = x2)
mean((y2 - predy2_pcr2)^2)
```

```{r}
set.seed(299)
pcr_fit2 <- train(x, y,
method = "pcr",
tuneGrid = data.frame(ncomp = 1:16),
trControl = ctrl1,
scale = TRUE)

predy2_pcr3 <- predict(pcr_fit2, newdata = x2)
mean((y2 - predy2_pcr3)^2)
```

```{r}
ggplot(pcr_fit, highlight = TRUE) + theme_bw()
```

We fit the PCR model using the function pcr(). PCR and PLS models are good for addressing
multicollinearity by maximizing the variance of predictor variables. 

##PLS using caret 
```{r}
set.seed(299)
pls_fit <- train(x, y,
method = "pls",
tuneGrid = data.frame(ncomp = 1:16),
trControl = ctrl1,
preProcess = c("center", "scale"))
predy2_pls2 <- predict(pls_fit, newdata = x2)
mean((y2 - predy2_pls2)^2)
```

```{r}
ggplot(pls_fit, highlight = TRUE)
```



#PLS
```{r}
set.seed(299)
pls_mod <- plsr(log_antibody ~ .,
data = training_data,
scale = TRUE,
validation = "CV")
summary(pls_mod)
```

```{r}
# plot cross-validated MSEP for PLS
validationplot(pls_mod, val.type = "MSEP", legendpos = "topright")
```

```{r}
# determine the optimal number of components
cv_mse <- RMSEP(pls_mod)
ncomp_cv <- which.min(cv_mse$val[1,,]) - 1
ncomp_cv
```

```{r}
# calculate test MSE
predy2_pls <- predict(pls_mod, newdata = testing_data,
ncomp = ncomp_cv)
mean((y2 - predy2_pls)^2)
```
We fit the PLS model using the function plsr(). This method uses a small number of 
the original inputs and considers out outcome, log antibody levels, making it more
predictive over PCR potentially. 




```{r}
set.seed(299)
enet_fit <- train(log_antibody ~ .,
data = training_data,
method = "glmnet",
tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
lambda = exp(seq(6, 0, length = 100))),
trControl = ctrl1)
```

We fit an elastic net model using the train() with method = "glmnet" function. Elastic
net models are good for effectively dealing with highly correlated groups of predictors. 

```{r}
resamp <- resamples(list(elastic_net = enet_fit,
pls = pls_fit,
pcr = pcr_fit))
summary(resamp)
```

```{r}
bwplot(resamp, metric = "RMSE")
```







## MARS - Naomi
```{r}

# Set seed for reproducibility
set.seed(299)

# Specify MARS grid - first attempt 
# mars_grid <- expand.grid(
#  degree = 1:3, # degree of interactions
#  nprune = 2:15  # no. of retained terms
# )

# MARS tuning grid - expanding grid 
mars_grid <- expand.grid(
  degree = 1:4,     # interaction degrees
  nprune = 2:20     # number of terms
)

# Train MARS model to predict log_antibody
mars.fit <- train(log_antibody ~ .,
                  data = training_data,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

# Plot CV performance
ggplot(mars.fit)


# Optimal parameters
mars.fit$bestTune

# Final model coefficients
mars_coef <- coef(mars.fit$finalModel)

mars_coef

```

To evaluate the optimal complexity of the MARS model, I performed a grid search across degrees 1 to 4 and a range of 2 to 20 terms (`nprune`). Cross-validation results showed that models with degree = 1 consistently achieved the lowest RMSE, and increasing the interaction degree did not lead to further improvements. 

As the interaction `degree` increased, RMSE values became more variable rather than decreasing steadily, suggesting there was no gain in prediction accuracy. The `nprune` range of 2 to 20 also appeared appropriate — RMSE decreased initially with more terms but then plateaued, indicating that adding further complexity would not significantly improve model performance. Based on this, I selected `degree = 1` as the optimal choice.

In other words, cross-validation results showed that a model with approximately 9 terms and no interactions (product degree = 1) minimized prediction error (RMSE ≈ 0.529). 

**Optimal parameters and final model coefficients**
The best-tuned model selected `degree = 1` and `nprune = 9`. Therefore, the final model includes 9 retained terms with no interaction effects.

Next, obtaining the test error:

```{r}

# Set seed for reproducibility
set.seed(299)

# Obtain response variable from testing data
y_testing <- testing_data$log_antibody

# Predict on test data using trained MARS model
y_pred_MARS <- predict(mars.fit, newdata = testing_data)

# Compute RMSE (Test Error)
test_rmse_mars <- sqrt(mean((y_testing - y_pred_MARS)^2))

# Print test RMSE
test_rmse_mars

```

Therefore, the test error (RMSE) is `0.5327718`.

Next, obtaining the training error:

```{r}

# Set seed for reproducibility
set.seed(299)

# Predict on training data using MARS model
y_pred_mars_train <- predict(mars.fit, newdata = training_data)

# Compute Training RMSE for MARS Model
train_rmse_mars <- sqrt(mean((training_data$log_antibody - y_pred_mars_train)^2))

# Print Training RMSE
print(train_rmse_mars)

```

Therefore, the training error (RMSE) is **0.5261998**.

## GAM - Naomi

I will proceed with constructing a GAM model, allowing us to mix non-linear and linear terms and build a model estimating the relationship between the outcome (`log_antibody`) and predictors in the provided dataset.
 
```{r}

# Set seed for reproducibility
set.seed(299)

# Fit a GAM model, using training data
gam_antibody <- gam(log_antibody ~ gender + race + smoking +
                      s(height) + s(weight) + s(bmi) +
                      diabetes + hypertension + s(SBP) + 
                      s(LDL) + s(time), 
                    data = training_data)

# Summary
summary(gam_antibody)

# Plot GAM 
plot(gam_antibody)


# Use train() to fit GAM Model

gam.fit <- train(
  log_antibody ~ gender + race + smoking +
    height + weight + bmi +
    diabetes + hypertension + SBP + 
    LDL + time,
  data = training_data,
  method = "gam",
  trControl = ctrl1
)

summary(gam.fit)

```

* Some of the predictors have estimated degrees of freedom (edf) greater than 1, specifically height, BMI, and time, indicating that they are nonlinear smooth functions. This is supported by the GAM predictor plots.

* Others appear approximately linear (edf ≈ 1), including weight, SBP, and LDL. 

* Among the nonlinear terms, BMI and time are highly significant (p < 2e-16), suggesting meaningful nonlinear associations with the log antibody levels.

* However, not all nonlinear terms with edf > 1 are statistically significant. i.e, height has an edf of 2.272 but a non-significant p-value (p = 0.3736).

* SBP is a linear term (edf = 1.00) and also statistically significant (p = 0.0182), suggesting a meaningful linear relationship. It is the only linear term that is statistically significant. 

Next, obtaining the training error:

```{r}

# Note I have not done test error
# Set seed for reproducibility
set.seed(299)

# Prediction using training data
y_train_pred_gam <- predict(gam_antibody, newdata = training_data)

# Training RMSE
train_rmse_gam <- sqrt(mean((training_data$log_antibody - y_train_pred_gam)^2))

print(train_rmse_gam)

```

Therefore, the training set error (RMSE) is **0.5312249**.


# Code for Comparison/Model selection

```{r}

# Compare models using cross-validated RMSE
resamp <- resamples(list(
  pls = pls_fit,
  pcr = pcr_fit,
  elastic_net = enet_fit,
  mars = mars.fit,
  gam = gam.fit))

# Print CV summary
summary(resamp)

# Plot RMSE comparison
bwplot(resamp, metric = "RMSE")


```

```{r}
class(pls_fit)
class(pcr_fit)
class(enet_fit)
class(mars.fit)
class(gam.fit)

```

